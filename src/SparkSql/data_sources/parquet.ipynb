{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520e9e77-5b18-445f-992e-b489da662eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77687bf6-9b88-46fb-8956-6b660b3be12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/08 11:44:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "session = SparkSession.builder.appName('parquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14a5b7d-623d-48fd-b000-559ef0627c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.read.json('/opt/bitnami/spark/data/people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c19612-8528-4335-a714-b6bd002b6bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.parquet('/opt/bitnami/spark/data/temp/people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00f0b50-0c80-4a42-b501-9d0ed79000c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_par = session.read.parquet('/opt/bitnami/spark/data/temp/people.parquet')\n",
    "\n",
    "df_par.createOrReplaceTempView('parquetFiles')\n",
    "\n",
    "teen = session.sql('select name from parquetFiles where age >= 13 and age <=19')\n",
    "teen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bcf75-3dbc-469d-ba58-09fe7eb18038",
   "metadata": {},
   "source": [
    "### Meging Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c3f8ad-b3f9-4c4d-9340-f32c4438771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5815d0bf-f19f-442a-8f28-cca555ac5256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|single_x|double_x|\n",
      "+--------+--------+\n",
      "|       0|       0|\n",
      "|       1|       1|\n",
      "|       2|       4|\n",
      "|       3|       9|\n",
      "|       4|      16|\n",
      "|       5|      25|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_o = session.createDataFrame(\n",
    "    sc.parallelize(range(6)).map(lambda x: Row(single_x=x, double_x=x**2))\n",
    ")\n",
    "df_o.write.parquet('/opt/bitnami/spark/data/temp/testable/key=1')\n",
    "df_o.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a0e532-9ff6-4c98-a459-e4c954baa674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|single_X|triple_x|\n",
      "+--------+--------+\n",
      "|       0|       0|\n",
      "|       1|       1|\n",
      "|       2|       8|\n",
      "|       3|      27|\n",
      "|       4|      64|\n",
      "|       5|     125|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t = session.createDataFrame(\n",
    "    sc.parallelize(range(6)).map(lambda x: Row(single_X=x, triple_x=x**3))\n",
    ")\n",
    "\n",
    "df_t.write.parquet('/opt/bitnami/spark/data/temp/testable/key=2')\n",
    "df_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82f055c2-3bc1-45f6-a754-d1131f0a1e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single_x: long (nullable = true)\n",
      " |-- double_x: long (nullable = true)\n",
      " |-- triple_x: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergeDf = session.read.option('mergeSchema', 'true').parquet('/opt/bitnami/spark/data/temp/testable')\n",
    "mergeDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b695c9fe-0496-4cfb-bfd9-0f6a3c814844",
   "metadata": {},
   "source": [
    "### Columnar encryption\n",
    "\n",
    "Since Spark 3.2, columnar encryption is supported for Parquet tables with Apache Parquet 1.12+.\n",
    "\n",
    "Parquet uses the envelope encryption practice, where file parts are encrypted with “data encryption keys” (DEKs), and the DEKs are encrypted with “master encryption keys” (MEKs). \n",
    "\n",
    "The DEKs are randomly generated by Parquet for each encrypted file/column. \n",
    "\n",
    "The MEKs are generated, stored and managed in a Key Management Service (KMS) of user’s choice. \n",
    "\n",
    "The Parquet Maven repository has a jar with a mock KMS implementation that allows to run column encryption and decryption using a spark-shell only, without deploying a KMS server (download the parquet-hadoop-tests.jar file and place it in the Spark jars folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c61cfe-6681-4de1-8a11-d091be747afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+---+\n",
      "|single_x|double_x|triple_x|key|\n",
      "+--------+--------+--------+---+\n",
      "|       0|       0|    NULL|  1|\n",
      "|       1|       1|    NULL|  1|\n",
      "|       2|       4|    NULL|  1|\n",
      "|       3|       9|    NULL|  1|\n",
      "|       4|      16|    NULL|  1|\n",
      "|       5|      25|    NULL|  1|\n",
      "|       0|    NULL|       0|  2|\n",
      "|       1|    NULL|       1|  2|\n",
      "|       2|    NULL|       8|  2|\n",
      "|       3|    NULL|      27|  2|\n",
      "|       4|    NULL|      64|  2|\n",
      "|       5|    NULL|     125|  2|\n",
      "+--------+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "560797e5-8618-4e78-afff-4c69aadc5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "mergeDf.write.\\\n",
    "option('parquet.encryption.column.keys', 'keyA:square').\\\n",
    "option('partition.encryption.footer.key', 'keyB')\\\n",
    ".parquet('/opt/bitnami/spark/data/temp/merged_encrypt.parquet.encrypted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeacea31-a113-4dcc-8d25-4d6502aee7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+---+\n",
      "|single_x|double_x|triple_x|key|\n",
      "+--------+--------+--------+---+\n",
      "|       0|       0|    NULL|  1|\n",
      "|       1|       1|    NULL|  1|\n",
      "|       4|      16|    NULL|  1|\n",
      "|       5|      25|    NULL|  1|\n",
      "|       0|    NULL|       0|  2|\n",
      "|       1|    NULL|       1|  2|\n",
      "|       4|    NULL|      64|  2|\n",
      "|       5|    NULL|     125|  2|\n",
      "|       2|       4|    NULL|  1|\n",
      "|       3|       9|    NULL|  1|\n",
      "|       2|    NULL|       8|  2|\n",
      "|       3|    NULL|      27|  2|\n",
      "+--------+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m = session.read.parquet('/opt/bitnami/spark/data/temp/merged_encrypt.parquet.encrypted')\n",
    "df_m.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
